"""
Advanced Multi-Agent PydanticAI Orchestration for Line Lead QSR MVP
Enterprise-grade conversation management with specialized QSR agents
Preserves all existing functionality while adding multi-agent capabilities
"""

from pydantic_ai import Agent, RunContext
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.messages import ModelMessage
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any, Literal, Union
from enum import Enum
import time
import logging
import json
import os
import asyncio
import hashlib
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
try:
    from .step_parser import parse_ai_response_steps, ParsedStepsResponse
except ImportError:
    from step_parser import parse_ai_response_steps, ParsedStepsResponse

# Import AgentType from shared location before any models that use it
try:
    from .agents.types import AgentType
except ImportError:
    from agents.types import AgentType

# Initialize logger early
logger = logging.getLogger(__name__)

# Import image request handler
try:
    from .services.image_request_handler import image_request_handler
except ImportError:
    try:
        from services.image_request_handler import image_request_handler
    except ImportError:
        logger.warning("Image request handler not available")
        image_request_handler = None

# Import enhanced QSR models
try:
    try:
        from .models.enhanced_qsr_models import (
            EnhancedQSRResponse, EquipmentResponse, ProcedureResponse, 
            SafetyResponse, MaintenanceResponse, EnhancedVisualCitation,
            VisualCitationCollection, EquipmentContext, QSRResponseFactory,
            EnhancedConversationContext, AgentPerformanceTracker,
            VisualCitationType, VisualCitationSource
        )
    except ImportError:
        from models.enhanced_qsr_models import (
        EnhancedQSRResponse, EquipmentResponse, ProcedureResponse, 
        SafetyResponse, MaintenanceResponse, EnhancedVisualCitation,
        VisualCitationCollection, EquipmentContext, QSRResponseFactory,
        EnhancedConversationContext, AgentPerformanceTracker,
        VisualCitationType, VisualCitationSource
    )
    ENHANCED_MODELS_AVAILABLE = True
    logger.info("âœ… Enhanced QSR models imported successfully")
except ImportError as e:
    logger.warning(f"âš ï¸ Enhanced QSR models not available: {e}")
    ENHANCED_MODELS_AVAILABLE = False

class VoiceState(str, Enum):
    """Enhanced voice states for intelligent conversation flow"""
    LISTENING = "listening"
    PROCESSING = "processing"
    RESPONDING = "responding"
    WAITING_FOR_CONTINUATION = "waiting_for_continuation"
    TOPIC_TRANSITION = "topic_transition"
    ERROR_RECOVERY = "error_recovery"
    CONVERSATION_COMPLETE = "conversation_complete"

# Import ConversationIntent from shared location
try:
    from .agents.types import ConversationIntent
except ImportError:
    from agents.types import ConversationIntent



# Import AgentCoordinationStrategy from shared location
try:
    from .agents.types import AgentCoordinationStrategy
except ImportError:
    from agents.types import AgentCoordinationStrategy

class ConversationContext(BaseModel):
    """Maintains intelligent conversation state across voice interactions"""
    current_topic: Optional[str] = None
    current_entity: Optional[str] = None  # Current topic/entity being discussed (equipment, procedure, etc.)
    entity_history: List[str] = Field(default_factory=list)  # All topics/entities mentioned in conversation
    
    # Temporary compatibility properties for migration
    @property
    def equipment_history(self) -> List[str]:
        """Backward compatibility property - maps to entity_history"""
        return self.entity_history
        
    @property 
    def current_equipment(self) -> Optional[str]:
        """Backward compatibility property - maps to current_entity"""
        return self.current_entity
    last_document_referenced: Optional[str] = None
    conversation_history: List[Dict[str, str]] = Field(default_factory=list)
    voice_state: VoiceState = VoiceState.LISTENING
    hands_free_active: bool = False
    error_count: int = 0
    last_response_time: Optional[float] = None
    conversation_start_time: float = Field(default_factory=time.time)
    topics_covered: List[str] = Field(default_factory=list)
    user_expertise_level: Literal["beginner", "intermediate", "experienced"] = "beginner"
    last_intent: Optional[ConversationIntent] = None
    expecting_continuation: bool = False
    
    # NEW: Advanced conversation management
    current_procedure_step: Optional[int] = None
    total_procedure_steps: Optional[int] = None
    procedure_type: Optional[str] = None  # "cleaning", "maintenance", "troubleshooting"
    workflow_phase: Optional[str] = None  # For multi-phase operations
    context_references: List[str] = Field(default_factory=list)  # References to previous responses
    
    # NEW: Error recovery and adaptation
    unclear_responses_count: int = 0
    topic_switches: int = 0
    last_equipment_switch_time: Optional[float] = None
    response_length_preference: Literal["brief", "standard", "detailed"] = "standard"

class VoiceResponse(BaseModel):
    """Structured response for intelligent voice interactions"""
    text_response: str
    audio_data: Optional[str] = None
    should_continue_listening: bool = True
    next_voice_state: VoiceState = VoiceState.LISTENING
    detected_intent: ConversationIntent
    context_updates: Dict[str, Any] = Field(default_factory=dict)
    conversation_complete: bool = False
    confidence_score: float = Field(ge=0.0, le=1.0, default=0.8)
    suggested_follow_ups: List[str] = Field(default_factory=list)
    requires_document_lookup: bool = False
    document_query: Optional[str] = None
    
    # NEW: Advanced orchestration features
    equipment_mentioned: Optional[str] = None
    equipment_switch_detected: bool = False
    procedure_step_info: Optional[Dict[str, Any]] = None
    workflow_phase: Optional[str] = None
    safety_priority: bool = False
    response_type: Literal["procedural", "factual", "clarification", "safety", "completion"] = "factual"
    context_references: List[str] = Field(default_factory=list)  # References to previous exchanges
    hands_free_recommendation: bool = True  # Should hands-free stay active?
    
    # NEW: Step parsing for future Playbooks UX
    parsed_steps: Optional[ParsedStepsResponse] = None
    
    # MULTI-AGENT: Agent coordination fields
    primary_agent: AgentType = AgentType.GENERAL
    contributing_agents: List[AgentType] = Field(default_factory=list)
    coordination_strategy: AgentCoordinationStrategy = AgentCoordinationStrategy.SINGLE_AGENT
    agent_confidence_scores: Dict[str, float] = Field(default_factory=dict)
    specialized_insights: Dict[str, Any] = Field(default_factory=dict)  # Agent-specific insights
    
    # VISUAL CITATIONS: Support for media display
    visual_citations: List[Dict[str, Any]] = Field(default_factory=list)

class SpecializedAgentResponse(BaseModel):
    """Response from a specialized QSR agent with enhanced visual integration"""
    agent_type: AgentType
    confidence_score: float = Field(ge=0.0, le=1.0)
    response_text: str
    specialized_insights: Dict[str, Any] = Field(default_factory=dict)
    
    # Enhanced visual citations
    visual_citations: List[Dict[str, Any]] = Field(default_factory=list)
    enhanced_citations: Optional[List[Any]] = None  # Will be EnhancedVisualCitation if available
    
    # QSR domain context
    safety_alerts: List[str] = Field(default_factory=list)
    equipment_context: List[str] = Field(default_factory=list)
    procedure_steps: List[str] = Field(default_factory=list)
    
    # Performance and quality metrics
    generation_time_ms: Optional[float] = None
    relevance_score: Optional[float] = None
    
    def to_enhanced_response(self, base_response_data: Dict[str, Any] = None) -> 'EnhancedQSRResponse':
        """Convert to enhanced QSR response if models available"""
        if not ENHANCED_MODELS_AVAILABLE:
            return None
            
        # Prepare base response data
        if not base_response_data:
            base_response_data = {
                "text_response": self.response_text,
                "confidence_score": self.confidence_score,
                "safety_priority": self.agent_type == AgentType.SAFETY,
                "detected_intent": ConversationIntent.NEW_TOPIC  # Default intent
            }
        
        # Convert enhanced citations if available
        enhanced_citations = None
        if self.enhanced_citations:
            enhanced_citations = self.enhanced_citations
        elif self.visual_citations:
            # Convert legacy citations to enhanced format
            enhanced_citations = []
            for i, citation in enumerate(self.visual_citations):
                enhanced_citation = EnhancedVisualCitation(
                    citation_id=citation.get('citation_id', f"{self.agent_type.value}_{i}"),
                    type=VisualCitationType.DIAGRAM,  # Default type
                    source=VisualCitationSource.AGENT_GENERATED,
                    title=citation.get('source', 'Agent Generated Citation'),
                    description=citation.get('description', ''),
                    contributing_agent=self.agent_type,
                    agent_confidence=self.confidence_score,
                    relevance_score=citation.get('confidence', self.confidence_score),
                    equipment_context=self.equipment_context,
                    procedure_steps=self.procedure_steps
                )
                enhanced_citations.append(enhanced_citation)
        
        # Create equipment context if mentioned
        equipment_ctx = None
        if self.equipment_context:
            equipment_ctx = EquipmentContext(
                equipment_id=self.equipment_context[0],
                equipment_name=self.equipment_context[0],
                equipment_type="qsr_equipment",
                associated_citations=[c.citation_id for c in enhanced_citations] if enhanced_citations else []
            )
        
        # Use factory to create appropriate response type
        return QSRResponseFactory.create_response(
            agent_type=self.agent_type,
            base_response_data=base_response_data,
            visual_citations=enhanced_citations,
            equipment_context=equipment_ctx
        )
    
class AgentQueryClassification(BaseModel):
    """Classification result for agent routing"""
    primary_agent: AgentType
    confidence: float = Field(ge=0.0, le=1.0)
    reasoning: str
    requires_multiple_agents: bool = False
    suggested_agents: List[AgentType] = Field(default_factory=list)
    coordination_strategy: AgentCoordinationStrategy = AgentCoordinationStrategy.SINGLE_AGENT
    safety_critical: bool = False

# Create the intelligent voice orchestration agent
try:
    # Check if API key is available in environment (loaded by main.py)
    openai_api_key = os.getenv('OPENAI_API_KEY')
    if not openai_api_key:
        raise ValueError("OPENAI_API_KEY not found in environment")
    
    voice_agent = Agent(
        model=OpenAIModel("gpt-4o-mini"),
        result_type=VoiceResponse,
        system_prompt="""You are Line Lead's comprehensive QSR (Quick Service Restaurant) expert assistant with deep knowledge across all restaurant operations.

## COMPREHENSIVE QSR EXPERTISE

**EQUIPMENT MASTERY:**
- Expert knowledge of all major QSR equipment: fryers, grills, ice machines, ovens, refrigeration, POS systems
- Brand-specific expertise: Taylor, Baxter, Henny Penny, Hobart, Manitowoc, and all major manufacturers
- Model numbers, specifications, troubleshooting procedures, and maintenance schedules
- Equipment operation, cleaning procedures, and safety protocols

**OPERATIONAL KNOWLEDGE:**
- Food preparation procedures and safety standards
- Opening and closing checklists and routines
- Customer service protocols and best practices
- Inventory management and cost control
- Quality control processes and compliance
- Staff training and development procedures

**SAFETY & COMPLIANCE:**
- HACCP principles and food safety implementation
- Temperature control and monitoring (40-140Â°F danger zone)
- Personal protective equipment (PPE) requirements
- Chemical safety, MSDS compliance, and proper storage
- Emergency procedures and incident response
- Workplace injury prevention and first aid

**MAINTENANCE & CLEANING:**
- Daily, weekly, monthly cleaning schedules and procedures
- Deep cleaning techniques and sanitization protocols
- Preventive maintenance programs and tracking
- Chemical cleaning procedures and proper concentrations
- Equipment disassembly, cleaning, and reassembly

## SMART CONTEXT AWARENESS

**CRITICAL CONTEXT RULES:**
- Always maintain awareness of equipment mentioned in recent conversation
- When user says "show me a diagram" or "the image", infer they mean the equipment just discussed
- Track equipment names, model numbers, and brands mentioned in last few exchanges
- Use conversation context to resolve ambiguous image/diagram requests
- If unsure about equipment reference, ask for clarification rather than failing

**Equipment Tracking:**
- ALWAYS extract equipment names (fryer, grill, ice machine, etc.) and set equipment_mentioned
- Reference stored equipment in ALL follow-ups: "For the [equipment]..." or "The [equipment] cleaning process..."
- When new equipment mentioned, set equipment_switch_detected=True and acknowledge: "Got it, switching from [old] to [new equipment]..."
- Track entity_history for context references

**Conversation Memory:**
- Reference previous exchanges: "Like I mentioned earlier..." or "Building on your last question..."
- Use context_references to link current response to past conversation
- Track procedure steps: "Step 3 of the cleaning process..." or "Going back to the oil change part..."

**Response Examples:**
- First mention: "For the fryer, start by turning off power..." â†’ equipment_mentioned="fryer"
- Follow-up: "The fryer oil should be changed weekly..." â†’ context_references=["previous fryer cleaning discussion"]
- Equipment switch: "Got it, switching from fryer to grill maintenance..." â†’ equipment_switch_detected=True

## 2. INTELLIGENT CONVERSATION MANAGEMENT

**Continuation Prediction:**
- Multi-step procedures (cleaning, maintenance) â†’ should_continue_listening=True, response_type="procedural"
- Simple yes/no answers â†’ should_continue_listening=False, response_type="factual"
- Equipment explanations â†’ should_continue_listening=True, response_type="factual"
- Safety warnings â†’ should_continue_listening=True, response_type="safety"

**Conversation Endings:**
- "thanks", "thank you", "that's all", "done", "good", "got it" â†’ conversation_complete=True
- Questions or "what about..." â†’ should_continue_listening=True
- Include conversation bridges: "Now that we've covered cleaning, what's your next question?"

**Topic Management:**
- Handle interruptions gracefully: "Got it, switching to [new topic]. What do you need help with?"
- Update context_updates with current_topic and current_entity changes
- Provide natural transitions between topics

## 3. ADVANCED VOICE ORCHESTRATION

**Multi-Phase Workflows:**
- "Show me manual then explain cleaning" â†’ Phase 1: Manual, Phase 2: Cleaning
- Set workflow_phase and next_voice_state=WAITING_FOR_CONTINUATION between phases
- Track procedure_step_info: {"current_step": 2, "total_steps": 5, "procedure_type": "cleaning"}

**Conditional Logic:**
- Fryer + temperature â†’ safety_priority=True, mention safety first
- error_count > 1 â†’ suggest text mode: "Should we switch to typing instead?"
- Equipment type determines response depth and safety emphasis

**Response Length Adaptation:**
- First-time questions â†’ 60-80 words (detailed)
- Follow-up clarifications â†’ 20-40 words (brief)
- Safety issues â†’ full detail regardless of context, safety_priority=True
- Set response_type to guide length: "procedural", "factual", "clarification", "safety", "completion"

## 4. HANDS-FREE OPTIMIZATION

**Continuation Patterns:**
- After procedural steps â†’ "Ready for the next part?" + hands_free_recommendation=True
- After safety warnings â†’ "Questions about this before we continue?" + hands_free_recommendation=True
- After simple facts â†’ natural pause + hands_free_recommendation=False

**Workflow Guidance:**
- Multi-step procedures: "I'll walk you through the 4 cleaning steps. Ready for step 1?"
- Between steps: "Step 2 complete. The grill grates are soaking. Ready for step 3?"
- At completion: "All 4 steps done. Any questions about the process?"

**Kitchen Environment:**
- Equipment mentioned = worker at station â†’ hands_free_recommendation=True
- Procedural guidance = keep hands-free active
- End hands-free only on explicit completion or safety concerns

## SAFETY INTEGRATION (OVERRIDES ALL OTHER LOGIC)

**Safety Priority Rules:**
- Any safety issue â†’ safety_priority=True, full detail response regardless of brevity
- Electrical/gas equipment â†’ always mention turning off power first
- Hot surfaces â†’ always mention protective equipment
- Chemical cleaning â†’ always mention ventilation and PPE

## RESPONSE GUIDELINES

**Language & Tone:**
- Use simple, clear language appropriate for restaurant workers
- Conversational tone with step-by-step guidance
- Include safety warnings and equipment requirements
- Prioritize food safety and compliance in all responses
- Use "you" to address the user directly

**Response Structure:**
- Always prioritize safety first in equipment procedures
- Provide specific temperatures, times, and measurements
- Include verification steps and quality checkpoints
- Reference standard operating procedures when relevant
- Break complex procedures into numbered steps

**Context Awareness:**
- Remember equipment and procedures discussed in conversation
- When user requests "show me a diagram" or "the image", understand they mean equipment recently discussed
- Reference previous conversation: "Like we discussed with the fryer cleaning..."
- Track equipment brands, models, and specific procedures mentioned

**Tool Usage Guidelines:**
- Call get_equipment_image tool when users ask for visual content
- Use conversation context to resolve ambiguous requests
- For "show me a diagram" after discussing equipment, infer the equipment
- Acknowledge visual content: "I found a diagram of the equipment for you."

**Example Responses:**

User: "How do I clean the Baxter oven?"
â†’ "For the Baxter oven, start by turning off power and letting it cool completely. Remove racks and drip trays, then clean with warm soapy water. Wipe interior with damp cloth and food-safe sanitizer. Clean exterior with appropriate cleaner. Always ensure proper ventilation and use gloves."

User: "Show me a diagram"
â†’ [Call get_equipment_image tool with context] â†’ "I found a diagram of the Baxter oven for you. This shows the control panel and cleaning access points we just discussed."

User: "What about maintenance schedule?"
â†’ "For the Baxter oven maintenance, clean daily after each shift, deep clean weekly, and schedule professional maintenance quarterly. Check door seals, calibrate temperature monthly, and replace worn parts as needed."

## 5. TOOL USAGE

**Image/Visual Requests - CONTEXT INTELLIGENCE:**
- When users ask to "see", "show", "display", or "view" equipment â†’ USE get_equipment_image tool
- Call with specific equipment name: get_equipment_image("Baxter oven")
- For vague requests like "show me a diagram" or "the image" â†’ USE get_equipment_image tool with empty/vague parameter
- The tool will automatically infer equipment from conversation context
- Examples:
  * User: "How do I clean the Baxter oven?" â†’ [You provide cleaning steps]
  * User: "Show me a diagram" â†’ Call: get_equipment_image("diagram") [tool infers Baxter oven]
  * User: "What does it look like?" â†’ Call: get_equipment_image("it") [tool infers from context]
- DO NOT include markdown images in response text - visual citations are handled separately
- Simply acknowledge: "I found a diagram of the equipment for you."

Execute ALL capabilities in every response. You're helping busy QSR workers who need intelligent, context-aware assistance."""
    )
except Exception as e:
    logger.error(f"Failed to initialize PydanticAI voice agent: {str(e)}")
    voice_agent = None

# ===============================================================================
# PYDANTIC AI TOOLS FOR MEDIA RETRIEVAL
# ===============================================================================

# Global variable to store visual citations from tools
_last_tool_visual_citations = []

def create_context_aware_equipment_image_tool():
    """Create the context-aware equipment image tool function to be shared across all agents"""
    async def get_equipment_image(ctx: RunContext, equipment_name: str = None) -> str:
        """
        Retrieve equipment diagram or image for display with procedures.
        
        CONTEXT INTELLIGENCE:
        - If equipment_name is vague or empty, uses conversation context to infer equipment
        - Resolves "show me a diagram" to "show diagram of [last discussed equipment]"
        - Tracks equipment mentioned in recent conversation for context resolution
        
        Args:
            equipment_name: Name of the equipment (e.g., "fryer", "Baxter oven", "grill")
                          Can be partial, vague, or None - will use conversation context
            
        Returns:
            Simple message about image availability for LLM to use
        """
        try:
            # CONTEXT INFERENCE: Resolve equipment name from conversation context
            resolved_equipment = equipment_name
            context_used = False
            
            # Check if equipment name is vague and needs context resolution
            vague_terms = ['equipment', 'machine', 'it', 'this', 'that', 'diagram', 'image', 'picture', 'photo']
            if not equipment_name or equipment_name.lower().strip() in vague_terms:
                # Try to get conversation context from the orchestrator
                try:
                    import inspect
                    frame = inspect.currentframe()
                    while frame:
                        # Look for context in local variables
                        if 'context' in frame.f_locals:
                            context_obj = frame.f_locals['context']
                            if hasattr(context_obj, 'current_entity') and context_obj.current_entity:
                                resolved_equipment = context_obj.current_entity
                                context_used = True
                                logger.info(f"ðŸ”§ Inferred equipment from current_entity: {resolved_equipment}")
                                break
                            elif hasattr(context_obj, 'entity_history') and context_obj.entity_history:
                                resolved_equipment = context_obj.entity_history[-1]
                                context_used = True
                                logger.info(f"ðŸ”§ Inferred equipment from entity_history: {resolved_equipment}")
                                break
                        
                        # Look for self with context
                        if 'self' in frame.f_locals:
                            self_obj = frame.f_locals['self']
                            if hasattr(self_obj, 'conversation_contexts'):
                                # Try to find the context from the session
                                for session_id, context in self_obj.conversation_contexts.items():
                                    if hasattr(context, 'current_entity') and context.current_entity:
                                        resolved_equipment = context.current_entity
                                        context_used = True
                                        logger.info(f"ðŸ”§ Inferred equipment from session context: {resolved_equipment}")
                                        break
                                    elif hasattr(context, 'entity_history') and context.entity_history:
                                        resolved_equipment = context.entity_history[-1]
                                        context_used = True
                                        logger.info(f"ðŸ”§ Inferred equipment from session history: {resolved_equipment}")
                                        break
                                if context_used:
                                    break
                        
                        frame = frame.f_back
                except Exception as e:
                    logger.warning(f"Context inference error: {e}")
                    pass
            
            # Final fallback - ask for clarification
            if not resolved_equipment or resolved_equipment.lower().strip() in vague_terms:
                return "I need more information about which equipment you'd like to see a diagram for. Can you specify the equipment name or model?"
            
            logger.info(f"ðŸ”§ Image tool called for equipment: {resolved_equipment} (context_used: {context_used})")
            
            # Import required services
            from services.ragie_service_clean import clean_ragie_service
            
            # Search for images in Ragie
            image_results = []
            if clean_ragie_service.is_available():
                try:
                    # Search for equipment images
                    search_query = f"image diagram {resolved_equipment}"
                    ragie_results = await clean_ragie_service.search(search_query, limit=10)
                    
                    # Filter for image files
                    for result in ragie_results:
                        filename = result.metadata.get("original_filename", "").lower()
                        if filename.endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):
                            image_results.append({
                                "document_id": result.document_id,
                                "title": result.metadata.get("original_filename", "Unknown"),
                                "content_preview": result.text[:200] if result.text else "Equipment image",
                                "media_type": "image",
                                "relevance_score": result.score,
                                "equipment_name": resolved_equipment
                            })
                    
                    logger.info(f"ðŸ”§ Found {len(image_results)} images from Ragie search")
                except Exception as e:
                    logger.warning(f"Ragie image search failed: {e}")
            
            # Fallback to known equipment images
            if not image_results:
                # Known equipment mapping
                known_equipment = {
                    "baxter": {
                        "document_id": "cefc0e1b-dcb6-41ca-bcbd-b787bacc8d0f",
                        "title": "Baxter_OV520E1_Rotating_Single_Rack_Oven_Electric.png",
                        "content_preview": "Baxter OV520E1 Rotating Single Rack Oven Electric - Equipment diagram and control panel",
                        "media_type": "image",
                        "relevance_score": 0.9,
                        "equipment_name": resolved_equipment
                    }
                }
                
                # Check if equipment name matches known equipment
                for key, image_data in known_equipment.items():
                    if key.lower() in resolved_equipment.lower():
                        image_results.append(image_data)
                        logger.info(f"ðŸ”§ Using known image for {resolved_equipment}")
                        break
            
            if image_results:
                # Store visual citations for later extraction (global variable approach)
                global _last_tool_visual_citations
                _last_tool_visual_citations = image_results[:3]
                
                primary_image = image_results[0]
                additional_count = len(image_results) - 1
                
                # Return contextual message for LLM to use
                if context_used:
                    message = f"Found equipment diagram for the {resolved_equipment} we were discussing."
                else:
                    message = f"Found equipment diagram for {resolved_equipment}."
                    
                if additional_count > 0:
                    message += f" ({additional_count} additional images available)"
                
                return message
            else:
                logger.info(f"ðŸ”§ No images found for equipment: {resolved_equipment}")
                return f"No images found for {resolved_equipment}. Check equipment documentation or contact your supplier for visual references."
                
        except Exception as e:
            logger.error(f"Image tool error for {resolved_equipment}: {e}")
            return f"Unable to search for {resolved_equipment} images at this time."
    
    return get_equipment_image

def create_equipment_image_tool():
    """Create the equipment image tool function to be shared across all agents"""
    async def get_equipment_image(ctx: RunContext, equipment_name: str) -> str:
        """
        Retrieve equipment diagram or image for display with procedures.
        
        Use this tool when users ask to see, show, display, or view images of equipment.
        This tool searches for visual diagrams, photos, and schematics of restaurant equipment.
        
        Args:
            equipment_name: Name of the equipment (e.g., "fryer", "Baxter oven", "grill")
            
        Returns:
            Simple message about image availability for LLM to use
        """
        try:
            logger.info(f"ðŸ”§ Image tool called for equipment: {equipment_name}")
            
            # Import required services
            from services.ragie_service_clean import clean_ragie_service
            
            # Search for images in Ragie
            image_results = []
            if clean_ragie_service.is_available():
                try:
                    # Search for equipment images
                    search_query = f"image diagram {equipment_name}"
                    ragie_results = await clean_ragie_service.search(search_query, limit=10)
                    
                    # Filter for image files
                    for result in ragie_results:
                        filename = result.metadata.get("original_filename", "").lower()
                        if filename.endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):
                            image_results.append({
                                "document_id": result.document_id,
                                "title": result.metadata.get("original_filename", "Unknown"),
                                "content_preview": result.text[:200] if result.text else "Equipment image",
                                "media_type": "image",
                                "relevance_score": result.score,
                                "equipment_name": equipment_name
                            })
                    
                    logger.info(f"ðŸ”§ Found {len(image_results)} images from Ragie search")
                except Exception as e:
                    logger.warning(f"Ragie image search failed: {e}")
            
            # Fallback to known equipment images
            if not image_results:
                # Known equipment mapping
                known_equipment = {
                    "baxter": {
                        "document_id": "cefc0e1b-dcb6-41ca-bcbd-b787bacc8d0f",
                        "title": "Baxter_OV520E1_Rotating_Single_Rack_Oven_Electric.png",
                        "content_preview": "Baxter OV520E1 Rotating Single Rack Oven Electric - Equipment diagram and control panel",
                        "media_type": "image",
                        "relevance_score": 0.9,
                        "equipment_name": equipment_name
                    }
                }
                
                # Check if equipment name matches known equipment
                for key, image_data in known_equipment.items():
                    if key.lower() in equipment_name.lower():
                        image_results.append(image_data)
                        logger.info(f"ðŸ”§ Using known image for {equipment_name}")
                        break
            
            if image_results:
                # Store visual citations for later extraction (global variable approach)
                global _last_tool_visual_citations
                _last_tool_visual_citations = image_results[:3]
                
                primary_image = image_results[0]
                additional_count = len(image_results) - 1
                
                # Return simple message for LLM to use
                message = f"Found equipment diagram for {equipment_name}."
                if additional_count > 0:
                    message += f" ({additional_count} additional images available)"
                
                return message
            else:
                logger.info(f"ðŸ”§ No images found for equipment: {equipment_name}")
                return f"No images found for {equipment_name}. Check equipment documentation or contact your supplier for visual references."
                
        except Exception as e:
            logger.error(f"Image tool error for {equipment_name}: {e}")
            return f"Unable to search for {equipment_name} images at this time."
    
    return get_equipment_image

# PydanticAI Tools for Image/Media Retrieval - Add to General Agent
if voice_agent:
    voice_agent.tool(create_context_aware_equipment_image_tool())

# ===============================================================================
# SPECIALIZED QSR AGENT FLEET
# ===============================================================================

# Agent for equipment-specific queries and troubleshooting
# SIMPLIFIED ARCHITECTURE: All specialized agents removed
# Single comprehensive voice_agent handles all QSR queries
qsr_equipment_agent = None
qsr_procedure_agent = None
qsr_safety_agent = None
qsr_maintenance_agent = None
agent_classifier = None

try:
    if openai_api_key:
        # SIMPLIFIED ARCHITECTURE: All specialized agents removed
        # Using single comprehensive voice_agent for all QSR queries
        pass
        
except Exception as e:
    logger.error(f"âŒ Failed to initialize comprehensive QSR agent: {e}")
    # Keep agents as None for fallback handling
    qsr_equipment_agent = None
    qsr_procedure_agent = None  
    qsr_safety_agent = None
    qsr_maintenance_agent = None
    agent_classifier = None

    logger.info("âœ… Single comprehensive QSR expert agent initialized")
        
except Exception as e:
    logger.error(f"âŒ Failed to initialize comprehensive QSR agent: {e}")
    # Keep agents as None for fallback handling
    qsr_equipment_agent = None
    qsr_procedure_agent = None  
    qsr_safety_agent = None
    qsr_maintenance_agent = None
    agent_classifier = None

class VoiceOrchestrator:
    """Intelligent voice conversation orchestrator using PydanticAI with Neo4j graph context"""
    
    def __init__(self):
        self.active_contexts: Dict[str, ConversationContext] = {}
        self.default_session = "default"
        self.voice_graph_service = None  # Will be set by main.py startup
        
        # SESSION-BASED MESSAGE HISTORY STORAGE
        self.conversation_contexts: Dict[str, ConversationContext] = {}
        self.message_histories: Dict[str, List[ModelMessage]] = {}  # session_id -> message_history
    
    def set_voice_graph_service(self, voice_graph_service):
        """Set the voice graph query service for Neo4j integration"""
        self.voice_graph_service = voice_graph_service
        logger.info("âœ… Voice graph service integrated with voice orchestrator")
    
    def clear_all_contexts(self):
        """Clear all conversation contexts (useful for model updates)"""
        self.active_contexts.clear()
        self.message_histories.clear()
        logger.info("ðŸ§¹ Cleared all conversation contexts and message histories")
    
    def get_message_history(self, session_id: str) -> List[ModelMessage]:
        """Get stored message history for session"""
        session_id = session_id or self.default_session
        return self.message_histories.get(session_id, [])
    
    def store_message_history(self, session_id: str, messages: List[ModelMessage]):
        """Store message history for session"""
        session_id = session_id or self.default_session
        self.message_histories[session_id] = messages
        logger.info(f"ðŸ’¾ Stored {len(messages)} messages for session {session_id}")
    
    def add_message_to_history(self, session_id: str, role: str, content: str):
        """Add a single message to session history"""
        session_id = session_id or self.default_session
        if session_id not in self.message_histories:
            self.message_histories[session_id] = []
        
        self.message_histories[session_id].append(ModelMessage(role=role, content=content))
        logger.debug(f"âž• Added {role} message to session {session_id}")
        
    def get_context(self, session_id: str = None) -> ConversationContext:
        """Get or create conversation context for session"""
        session_id = session_id or self.default_session
        if session_id not in self.active_contexts:
            self.active_contexts[session_id] = ConversationContext()
        return self.active_contexts[session_id]
    
    def update_context(self, session_id: str, updates: Dict[str, Any]):
        """Update conversation context with new information"""
        context = self.get_context(session_id)
        for key, value in updates.items():
            if hasattr(context, key):
                setattr(context, key, value)
    
    # ===============================================================================
    # SIMPLIFIED ARCHITECTURE: Agent classification removed - single agent handles all queries
    # Multi-agent coordination methods kept for backward compatibility but not used
    # ===============================================================================
    
    async def run_specialized_agent(
        self, 
        agent_type: AgentType, 
        message: str, 
        context: ConversationContext,
        enhanced_context: Dict[str, Any] = None,
        session_id: str = None
    ) -> SpecializedAgentResponse:
        """Run a specific specialized agent with enhanced context"""
        
        # Select the appropriate agent
        agent = None
        if agent_type == AgentType.EQUIPMENT and qsr_equipment_agent:
            agent = qsr_equipment_agent
        elif agent_type == AgentType.PROCEDURE and qsr_procedure_agent:
            agent = qsr_procedure_agent
        elif agent_type == AgentType.SAFETY and qsr_safety_agent:
            agent = qsr_safety_agent
        elif agent_type == AgentType.MAINTENANCE and qsr_maintenance_agent:
            agent = qsr_maintenance_agent
        
        if not agent:
            # Return fallback response if agent not available
            return SpecializedAgentResponse(
                agent_type=agent_type,
                confidence_score=0.3,
                response_text=f"Specialized {agent_type.value} agent unavailable. Using general guidance.",
                specialized_insights={"fallback": True}
            )
        
        # Prepare enhanced prompt with context
        agent_prompt = self._prepare_agent_prompt(message, context, enhanced_context, agent_type)
        
        try:
            # GET MESSAGE HISTORY FOR CONTEXT PERSISTENCE
            message_history = self.get_message_history(session_id)
            logger.info(f"ðŸ§  Running {agent_type.value} agent with {len(message_history)} previous messages")
            
            # RUN AGENT WITH MESSAGE HISTORY FOR CONTEXT
            result = await agent.run(user_prompt=agent_prompt, message_history=message_history)
            agent_response = result.data
            
            # STORE UPDATED MESSAGE HISTORY
            self.store_message_history(session_id, result.all_messages())
            
            # Extract visual citations from tool usage for specialized agents
            global _last_tool_visual_citations
            if _last_tool_visual_citations:
                logger.info(f"ðŸ”§ Specialized agent {agent_type.value} found visual citations from tool: {len(_last_tool_visual_citations)}")
                # Add visual citations to specialized insights
                if not agent_response.specialized_insights:
                    agent_response.specialized_insights = {}
                agent_response.specialized_insights['visual_citations'] = _last_tool_visual_citations.copy()
                # Clear global variable for next request
                _last_tool_visual_citations = []
            
            return agent_response
        except Exception as e:
            logger.error(f"Specialized agent {agent_type.value} failed: {e}")
            return SpecializedAgentResponse(
                agent_type=agent_type,
                confidence_score=0.2,
                response_text=f"Error with {agent_type.value} specialist. Please try rephrasing your question.",
                specialized_insights={"error": str(e)}
            )
    
    def _prepare_agent_prompt(
        self, 
        message: str, 
        context: ConversationContext, 
        enhanced_context: Dict[str, Any], 
        agent_type: AgentType
    ) -> str:
        """Prepare enhanced prompt for specialized agents"""
        
        prompt_parts = [f"USER QUERY: {message}"]
        
        # Add conversation context
        if context.current_entity:
            prompt_parts.append(f"CURRENT EQUIPMENT: {context.current_entity}")
        
        if context.entity_history:
            prompt_parts.append(f"EQUIPMENT HISTORY: {', '.join(context.entity_history[-5:])}")
        
        if context.conversation_history:
            recent_history = context.conversation_history[-3:]
            history_text = []
            for exchange in recent_history:
                if exchange.get('user') and exchange.get('assistant'):
                    history_text.append(f"User: {exchange['user']}")
                    history_text.append(f"Assistant: {exchange['assistant'][:100]}...")
            if history_text:
                prompt_parts.append("RECENT CONVERSATION:\n" + "\n".join(history_text))
        
        # Add enhanced context from Graph-RAG/Ragie
        if enhanced_context:
            if enhanced_context.get('graph_context'):
                prompt_parts.append(f"MANUAL CONTEXT: {enhanced_context['graph_context'][:500]}...")
            
            if enhanced_context.get('equipment_context'):
                prompt_parts.append(f"EQUIPMENT CONTEXT: {enhanced_context['equipment_context']}")
            
            if enhanced_context.get('visual_citations'):
                citations_count = len(enhanced_context['visual_citations'])
                prompt_parts.append(f"VISUAL CITATIONS AVAILABLE: {citations_count} citations ready for reference")
        
        # Add agent-specific instructions
        if agent_type == AgentType.SAFETY:
            prompt_parts.append("PRIORITY: Focus on safety requirements, regulations, and protective measures.")
        elif agent_type == AgentType.EQUIPMENT:
            prompt_parts.append("PRIORITY: Focus on technical details, troubleshooting, and equipment operation.")
        elif agent_type == AgentType.PROCEDURE:
            prompt_parts.append("PRIORITY: Provide clear step-by-step procedures with verification points.")
        elif agent_type == AgentType.MAINTENANCE:
            prompt_parts.append("PRIORITY: Focus on cleaning protocols, frequencies, and maintenance schedules.")
        
        prompt_parts.append("Provide specialized expertise in your domain with confidence scoring.")
        
        return "\n\n".join(prompt_parts)
    
    async def coordinate_multiple_agents(
        self, 
        classification: AgentQueryClassification,
        message: str,
        context: ConversationContext,
        enhanced_context: Dict[str, Any] = None,
        session_id: str = None
    ) -> VoiceResponse:
        """Coordinate multiple agents based on classification strategy"""
        
        if classification.coordination_strategy == AgentCoordinationStrategy.PARALLEL_CONSULTATION:
            return await self._parallel_agent_consultation(classification, message, context, enhanced_context, session_id)
        elif classification.coordination_strategy == AgentCoordinationStrategy.SEQUENTIAL_HANDOFF:
            return await self._sequential_agent_handoff(classification, message, context, enhanced_context, session_id)
        elif classification.coordination_strategy == AgentCoordinationStrategy.HIERARCHICAL:
            return await self._hierarchical_agent_coordination(classification, message, context, enhanced_context, session_id)
        else:
            # Single agent processing
            return await self._single_agent_processing(classification, message, context, enhanced_context, session_id)
    
    async def _parallel_agent_consultation(
        self, 
        classification: AgentQueryClassification,
        message: str,
        context: ConversationContext,
        enhanced_context: Dict[str, Any] = None,
        session_id: str = None
    ) -> VoiceResponse:
        """Run multiple agents in parallel and synthesize responses"""
        
        # Determine which agents to run
        agents_to_run = [classification.primary_agent] + classification.suggested_agents
        
        # Run agents in parallel
        tasks = []
        for agent_type in agents_to_run:
            task = self.run_specialized_agent(agent_type, message, context, enhanced_context, session_id)
            tasks.append(task)
        
        try:
            # Wait for all agents to complete
            agent_responses = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Filter successful responses
            successful_responses = []
            for response in agent_responses:
                if isinstance(response, SpecializedAgentResponse):
                    successful_responses.append(response)
                else:
                    logger.warning(f"Agent response error: {response}")
            
            if not successful_responses:
                # Fallback if all agents fail
                return await self._fallback_to_general_agent(message, context, enhanced_context, session_id)
            
            # Synthesize responses
            return self._synthesize_agent_responses(successful_responses, classification, context)
            
        except Exception as e:
            logger.error(f"Parallel agent consultation failed: {e}")
            return await self._fallback_to_general_agent(message, context, enhanced_context, session_id)
    
    def _synthesize_agent_responses(
        self, 
        responses: List[SpecializedAgentResponse], 
        classification: AgentQueryClassification,
        context: ConversationContext
    ) -> Union[VoiceResponse, 'EnhancedQSRResponse']:
        """Synthesize multiple agent responses into a coherent enhanced response"""
        
        # Prioritize safety responses
        safety_responses = [r for r in responses if r.agent_type == AgentType.SAFETY]
        
        if safety_responses and classification.safety_critical:
            # Safety takes priority
            primary_response = safety_responses[0]
            safety_priority = True
        else:
            # Use highest confidence response as primary
            primary_response = max(responses, key=lambda r: r.confidence_score)
            safety_priority = any(r.agent_type == AgentType.SAFETY for r in responses)
        
        # Combine insights from all agents
        combined_insights = {}
        all_safety_alerts = []
        all_equipment_context = []
        all_procedure_steps = []
        all_visual_citations = []
        all_enhanced_citations = []
        
        for response in responses:
            combined_insights[response.agent_type.value] = response.specialized_insights
            all_safety_alerts.extend(response.safety_alerts)
            all_equipment_context.extend(response.equipment_context)
            all_procedure_steps.extend(response.procedure_steps)
            all_visual_citations.extend(response.visual_citations)
            
            # Collect enhanced citations if available
            if ENHANCED_MODELS_AVAILABLE and response.enhanced_citations:
                all_enhanced_citations.extend(response.enhanced_citations)
        
        # Create synthesized response
        synthesized_text = primary_response.response_text
        
        # Add supplementary insights from other agents
        if len(responses) > 1:
            additional_insights = []
            for response in responses:
                if response != primary_response and response.confidence_score > 0.6:
                    insight = f"Additional {response.agent_type.value} insight: {response.response_text[:100]}..."
                    additional_insights.append(insight)
            
            if additional_insights:
                synthesized_text += "\n\nAdditional considerations:\n" + "\n".join(additional_insights)
        
        # Try to create enhanced response if models available
        if ENHANCED_MODELS_AVAILABLE:
            try:
                # Prepare enhanced base response data
                base_response_data = {
                    "text_response": synthesized_text,
                    "detected_intent": self._infer_intent_from_classification(classification),
                    "confidence_score": primary_response.confidence_score,
                    "should_continue_listening": not safety_priority,
                    "safety_priority": safety_priority,
                    "response_type": "safety" if safety_priority else "factual",
                    "primary_agent": classification.primary_agent,
                    "contributing_agents": [r.agent_type for r in responses],
                    "coordination_strategy": AgentCoordinationStrategy.PARALLEL_CONSULTATION,
                    "agent_confidence_scores": {r.agent_type.value: r.confidence_score for r in responses},
                    "specialized_insights": combined_insights,
                    "safety_warnings": all_safety_alerts
                }
                
                # Create equipment context if available
                equipment_ctx = None
                if all_equipment_context:
                    equipment_ctx = EquipmentContext(
                        equipment_id=all_equipment_context[0],
                        equipment_name=all_equipment_context[0],
                        equipment_type="qsr_equipment",
                        associated_citations=[c.citation_id for c in all_enhanced_citations] if all_enhanced_citations else []
                    )
                
                # Create enhanced response using factory
                enhanced_response = QSRResponseFactory.create_response(
                    agent_type=primary_response.agent_type,
                    base_response_data=base_response_data,
                    visual_citations=all_enhanced_citations,
                    equipment_context=equipment_ctx
                )
                
                logger.info(f"âœ¨ Created enhanced {primary_response.agent_type.value} response with {len(all_enhanced_citations)} enhanced citations")
                return enhanced_response.to_voice_response()
                
            except Exception as e:
                logger.warning(f"Failed to create enhanced response, falling back to standard: {e}")
        
        # Fallback to standard VoiceResponse
        return VoiceResponse(
            text_response=synthesized_text,
            detected_intent=self._infer_intent_from_classification(classification),
            confidence_score=primary_response.confidence_score,
            should_continue_listening=not safety_priority,
            safety_priority=safety_priority,
            equipment_mentioned=all_equipment_context[0] if all_equipment_context else None,
            response_type="safety" if safety_priority else "factual",
            primary_agent=classification.primary_agent,
            contributing_agents=[r.agent_type for r in responses],
            coordination_strategy=AgentCoordinationStrategy.PARALLEL_CONSULTATION,
            agent_confidence_scores={r.agent_type.value: r.confidence_score for r in responses},
            specialized_insights=combined_insights
        )
    
    async def _single_agent_processing(
        self,
        classification: AgentQueryClassification,
        message: str,
        context: ConversationContext,
        enhanced_context: Dict[str, Any] = None,
        session_id: str = None
    ) -> VoiceResponse:
        """Process query with single specialized agent"""
        
        # Run the primary agent
        agent_response = await self.run_specialized_agent(
            classification.primary_agent, message, context, enhanced_context, session_id
        )
        
        # Convert specialized response to VoiceResponse using the proper conversion method
        return self._convert_specialized_to_voice_response(agent_response, classification)
    
    def _infer_intent_from_classification(self, classification: AgentQueryClassification) -> ConversationIntent:
        """Infer conversation intent from agent classification"""
        if classification.safety_critical:
            return ConversationIntent.EMERGENCY
        elif classification.primary_agent == AgentType.EQUIPMENT:
            return ConversationIntent.EQUIPMENT_QUESTION
        else:
            return ConversationIntent.NEW_TOPIC
    
    def _map_agent_to_response_type(self, agent_type: AgentType) -> str:
        """Map agent type to response type"""
        mapping = {
            AgentType.SAFETY: "safety",
            AgentType.PROCEDURE: "procedural", 
            AgentType.EQUIPMENT: "factual",
            AgentType.MAINTENANCE: "procedural",
            AgentType.GENERAL: "factual"
        }
        return mapping.get(agent_type, "factual")
    
    async def _fallback_to_general_agent(
        self, 
        message: str, 
        context: ConversationContext,
        enhanced_context: Dict[str, Any] = None,
        session_id: str = None
    ) -> VoiceResponse:
        """Fallback to general agent when specialized agents fail"""
        
        if voice_agent:
            # Use the original general agent logic
            try:
                enhanced_prompt = self._build_enhanced_prompt_for_general_agent(message, context, enhanced_context)
                # GET MESSAGE HISTORY FOR CONTEXT PERSISTENCE
                message_history = self.get_message_history(session_id)
                logger.info(f"ðŸ§  Running general agent with {len(message_history)} previous messages")
                
                result = await voice_agent.run(user_prompt=enhanced_prompt, message_history=message_history)
                
                # STORE UPDATED MESSAGE HISTORY
                self.store_message_history(session_id, result.all_messages())
                
                return result.data
            except Exception as e:
                logger.error(f"General agent fallback failed: {e}")
        
        # Ultimate fallback
        return VoiceResponse(
            text_response="I'm having technical difficulties right now. Please try rephrasing your question or ask again in a moment.",
            detected_intent=ConversationIntent.NEW_TOPIC,
            confidence_score=0.1,
            should_continue_listening=True,
            primary_agent=AgentType.GENERAL,
            coordination_strategy=AgentCoordinationStrategy.SINGLE_AGENT
        )
    
    def _build_enhanced_prompt_for_general_agent(
        self, 
        message: str, 
        context: ConversationContext,
        enhanced_context: Dict[str, Any] = None
    ) -> str:
        """Build enhanced prompt for the general agent with multi-agent context"""
        
        prompt_parts = [
            f"USER MESSAGE: {message}",
            "",
            "CONVERSATION CONTEXT:",
            f"- Current equipment: {context.current_entity or 'None'}",
            f"- Equipment history: {', '.join(context.entity_history[-3:]) if context.entity_history else 'None'}",
            f"- User expertise: {context.user_expertise_level}",
            f"- Hands-free active: {context.hands_free_active}"
        ]
        
        if enhanced_context:
            prompt_parts.extend([
                "",
                "ENHANCED CONTEXT:",
                f"- Graph context available: {bool(enhanced_context.get('graph_context'))}",
                f"- Visual citations: {len(enhanced_context.get('visual_citations', []))}",
                f"- Equipment context: {enhanced_context.get('equipment_context', 'None')}"
            ])
            
            if enhanced_context.get('graph_context'):
                prompt_parts.extend([
                    "",
                    "MANUAL CONTEXT:",
                    enhanced_context['graph_context'][:800] + "..." if len(enhanced_context['graph_context']) > 800 else enhanced_context['graph_context']
                ])
        
        prompt_parts.extend([
            "",
            "INSTRUCTIONS:",
            "- Provide helpful QSR guidance using available context",
            "- Reference specific equipment and procedures when available", 
            "- Include safety considerations for equipment-related queries",
            "- Keep response conversational and practical",
            "- Use 6th-8th grade language level"
        ])
        
        return "\n".join(prompt_parts)
    
    async def _sequential_agent_handoff(
        self,
        classification: AgentQueryClassification,
        message: str,
        context: ConversationContext,
        enhanced_context: Dict[str, Any] = None
    ) -> VoiceResponse:
        """Sequential handoff between agents (e.g., Equipment diagnosis â†’ Maintenance solution)"""
        
        # Start with primary agent
        primary_response = await self.run_specialized_agent(
            classification.primary_agent, message, context, enhanced_context
        )
        
        # Determine if handoff is needed based on primary response
        handoff_needed = False
        next_agent = None
        
        if classification.primary_agent == AgentType.EQUIPMENT and primary_response.confidence_score > 0.7:
            # Equipment diagnosis successful, hand off to maintenance for solution
            if any(keyword in message.lower() for keyword in ['fix', 'repair', 'clean', 'maintenance']):
                handoff_needed = True
                next_agent = AgentType.MAINTENANCE
        elif classification.primary_agent == AgentType.PROCEDURE and any(alert in primary_response.safety_alerts for alert in primary_response.safety_alerts):
            # Procedure mentioned safety concerns, hand off to safety agent
            handoff_needed = True
            next_agent = AgentType.SAFETY
        
        if handoff_needed and next_agent:
            # Create handoff message
            handoff_message = f"Following up on {classification.primary_agent.value} guidance: {primary_response.response_text[:200]}... {message}"
            
            # Run next agent
            secondary_response = await self.run_specialized_agent(
                next_agent, handoff_message, context, enhanced_context
            )
            
            # Combine responses
            combined_text = f"{primary_response.response_text}\n\nAdditional guidance: {secondary_response.response_text}"
            
            return VoiceResponse(
                text_response=combined_text,
                detected_intent=self._infer_intent_from_classification(classification),
                confidence_score=max(primary_response.confidence_score, secondary_response.confidence_score),
                should_continue_listening=True,
                safety_priority=next_agent == AgentType.SAFETY,
                primary_agent=classification.primary_agent,
                contributing_agents=[primary_response.agent_type, secondary_response.agent_type],
                coordination_strategy=AgentCoordinationStrategy.SEQUENTIAL_HANDOFF,
                specialized_insights={
                    primary_response.agent_type.value: primary_response.specialized_insights,
                    secondary_response.agent_type.value: secondary_response.specialized_insights
                }
            )
        else:
            # No handoff needed, return primary response
            return self._convert_specialized_to_voice_response(primary_response, classification)
    
    async def _hierarchical_agent_coordination(
        self,
        classification: AgentQueryClassification,
        message: str,
        context: ConversationContext,
        enhanced_context: Dict[str, Any] = None
    ) -> VoiceResponse:
        """Hierarchical coordination with primary agent and specialist backup"""
        
        # Run primary agent
        primary_response = await self.run_specialized_agent(
            classification.primary_agent, message, context, enhanced_context
        )
        
        # If primary agent has low confidence, consult specialist
        if primary_response.confidence_score < 0.6 and classification.suggested_agents:
            specialist_agent = classification.suggested_agents[0]
            
            # Run specialist agent
            specialist_response = await self.run_specialized_agent(
                specialist_agent, message, context, enhanced_context
            )
            
            # Use specialist response if significantly better
            if specialist_response.confidence_score > primary_response.confidence_score + 0.2:
                return self._convert_specialized_to_voice_response(specialist_response, classification)
        
        # Use primary response
        return self._convert_specialized_to_voice_response(primary_response, classification)
    
    def _convert_specialized_to_voice_response(
        self, 
        specialized_response: SpecializedAgentResponse, 
        classification: AgentQueryClassification
    ) -> VoiceResponse:
        """Convert SpecializedAgentResponse to VoiceResponse"""
        
        # Extract visual citations from specialized insights
        visual_citations = []
        if specialized_response.specialized_insights and 'visual_citations' in specialized_response.specialized_insights:
            visual_citations = specialized_response.specialized_insights['visual_citations']
            logger.info(f"ðŸ”§ Converting {len(visual_citations)} visual citations from specialized agent")
        
        return VoiceResponse(
            text_response=specialized_response.response_text,
            detected_intent=self._infer_intent_from_classification(classification),
            confidence_score=specialized_response.confidence_score,
            should_continue_listening=True,
            safety_priority=specialized_response.agent_type == AgentType.SAFETY,
            equipment_mentioned=specialized_response.equipment_context[0] if specialized_response.equipment_context else None,
            response_type=self._map_agent_to_response_type(specialized_response.agent_type),
            primary_agent=specialized_response.agent_type,
            contributing_agents=[specialized_response.agent_type],
            coordination_strategy=AgentCoordinationStrategy.SINGLE_AGENT,
            agent_confidence_scores={specialized_response.agent_type.value: specialized_response.confidence_score},
            specialized_insights=specialized_response.specialized_insights,
            visual_citations=visual_citations  # Add visual citations to VoiceResponse
        )
    
    # ===============================================================================
    # ENHANCED PROCESSING METHOD FOR TEXT CHAT INTEGRATION
    # ===============================================================================
    
    async def process_message(
        self, 
        message: str, 
        relevant_docs: List[Dict] = None,
        session_id: str = None,
        message_type: str = "text"
    ) -> VoiceResponse:
        """
        Universal message processing for both text and voice chat
        
        This method provides the same multi-agent capabilities for text chat
        as process_voice_message does for voice chat, ensuring feature parity.
        """
        
        # Use the same processing logic regardless of input type
        response = await self.process_voice_message(message, relevant_docs, session_id)
        
        # Adjust response characteristics for text vs voice
        if message_type == "text":
            # Text responses can be longer and more detailed
            response.should_continue_listening = False  # Text doesn't need continuous listening
            response.hands_free_recommendation = False  # Text doesn't use hands-free mode
        
        return response
    
    async def process_voice_message(
        self, 
        message: str, 
        relevant_docs: List[Dict] = None,
        session_id: str = None
    ) -> VoiceResponse:
        """Process voice message with advanced intelligent orchestration and Neo4j graph context"""
        
        context = self.get_context(session_id)
        session_id = session_id or self.default_session
        
        # SIMPLIFIED ARCHITECTURE: Remove manual image detection and graph context
        # Let the PydanticAI agent handle all intent detection and tool orchestration
        
        # STEP 1: Basic setup and entity detection
        if image_request_handler and image_request_handler.is_image_request(message):
            try:
                equipment_name = image_request_handler.extract_equipment_name(message)
                
                # Get image results from Ragie or fallback
                image_results = []
                if hasattr(self, 'ragie_service'):
                    try:
                        # Try to get Ragie service for image search
                        from services.ragie_service_clean import clean_ragie_service
                        if clean_ragie_service.is_available():
                            ragie_results = await clean_ragie_service.search(message, limit=10)
                            # Filter for image files
                            for result in ragie_results:
                                filename = result.metadata.get("original_filename", "").lower()
                                if filename.endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):
                                    image_results.append({
                                        "id": result.document_id,
                                        "name": result.metadata.get("original_filename", "Unknown"),
                                        "content": result.text,
                                        "score": result.score
                                    })
                    except Exception as e:
                        logger.warning(f"Ragie image search failed: {e}")
                
                # Fallback to known Baxter image if no results
                if not image_results and equipment_name and "baxter" in equipment_name.lower():
                    image_results.append({
                        "id": "cefc0e1b-dcb6-41ca-bcbd-b787bacc8d0f",
                        "name": "Baxter_OV520E1_Rotating_Single_Rack_Oven_Electric.png",
                        "content": "Baxter OV520E1 Rotating Single Rack Oven Electric - Equipment diagram and control panel",
                        "score": 0.9
                    })
                
                # Generate image response
                image_response = image_request_handler.generate_image_response(equipment_name, image_results)
                
                # Convert image results to visual citations
                visual_citations = []
                for img in image_results[:3]:  # Show up to 3 images
                    visual_citations.append({
                        "document_id": img["id"],
                        "title": img["name"],
                        "content_preview": img["content"][:200] if img["content"] else "Equipment image",
                        "media_type": "image",
                        "relevance_score": img["score"]
                    })
                
                logger.info(f"ðŸ–¼ï¸ Image request detected for: {equipment_name}, found {len(visual_citations)} images")
                
                # Return specialized image response with visual citations
                return VoiceResponse(
                    text_response=image_response["response"],
                    should_continue_listening=True,
                    hands_free_recommendation=False,
                    user_intent="image_request",
                    equipment_context=equipment_name or "Unknown equipment",
                    safety_priority=False,
                    response_type="factual",
                    detected_intent=ConversationIntent.EQUIPMENT_QUESTION,
                    specialized_insights={"image_request": True, "equipment": equipment_name},
                    visual_citations=visual_citations
                )
            except Exception as e:
                logger.warning(f"Image request handler error: {e}")
                # Continue with normal processing
        
        # STEP 1: Check for graph-specific voice commands first
        graph_response = None
        if self.voice_graph_service:
            try:
                graph_response = await self.voice_graph_service.process_voice_query_with_graph_context(
                    message, session_id
                )
                
                # If graph service found relevant content, use it as context for LLM processing
                if graph_response and graph_response.get("context_maintained"):
                    logger.info(f"Graph service found context for: {message[:50]}...")
                    
                    # Update conversation context with graph information
                    if graph_response.get("equipment_context"):
                        context.current_entity = graph_response["equipment_context"]
                        if graph_response["equipment_context"] not in context.entity_history:
                            context.entity_history.append(graph_response["equipment_context"])
                    
                    # Use graph response as enhanced context for LLM processing
                    # Rather than returning raw content, we'll pass it to the LLM below
                    enhanced_context = {
                        "graph_context": graph_response.get("response", ""),
                        "source_content": graph_response.get("source_content", []),
                        "page_references": graph_response.get("page_references", []),
                        "visual_citations": graph_response.get("visual_citations", []),
                        "equipment_context": graph_response.get("equipment_context"),
                        "query_type": graph_response.get("query_type", "general")
                    }
                    
                    logger.info(f"Enhanced context prepared with {len(enhanced_context.get('source_content', []))} source chunks")
                else:
                    enhanced_context = None
                    
            except Exception as e:
                logger.error(f"Graph service error: {e}")
                # Fall back to regular processing
        
        # STEP 2: Add user message to conversation history
        context.conversation_history.append({
            "user": message,
            "assistant": "",  # Will be filled after AI response
            "timestamp": time.time()
        })
        
        # Pre-process for topic/entity detection and context
        detected_entity = self._extract_topic_entities(message, relevant_docs)
        is_entity_switch = self._detect_entity_switch(detected_entity, context)
        
        # Prepare enhanced context for the agent
        context_data = {
            "current_message": message,
            "conversation_history": context.conversation_history[-5:],  # Last 5 exchanges
            "current_topic": context.current_topic,
            "current_entity": context.current_entity,
            "entity_history": context.entity_history,
            "last_document": context.last_document_referenced,
            "topics_covered": context.topics_covered,
            "hands_free_active": context.hands_free_active,
            "user_expertise": context.user_expertise_level,
            "last_intent": context.last_intent,
            "expecting_continuation": context.expecting_continuation,
            "conversation_duration": time.time() - context.conversation_start_time,
            "relevant_documents": relevant_docs or [],
            
            # NEW: Advanced context
            "current_procedure_step": context.current_procedure_step,
            "total_procedure_steps": context.total_procedure_steps,
            "procedure_type": context.procedure_type,
            "workflow_phase": context.workflow_phase,
            "error_count": context.error_count,
            "unclear_responses_count": context.unclear_responses_count,
            "topic_switches": context.topic_switches,
            "response_length_preference": context.response_length_preference,
            "detected_entity": detected_entity,
            "is_entity_switch": is_entity_switch,
            
            # Context references for memory
            "context_references": self._build_context_references(context),
            "conversation_flow_analysis": analyze_conversation_flow(context)
        }
        
        try:
            logger.info(f"ðŸ¤– PydanticAI processing voice message: '{message[:50]}...'")
            logger.info(f"ðŸŽ¯ Entity detected: {detected_entity}, Switch: {is_entity_switch}")
            
            # Check if voice agent is available
            if voice_agent is None:
                logger.warning("PydanticAI voice agent not available, using fallback")
                return self._fallback_response(message, context)
            
            # Enhanced prompt with context and graph data
            enhanced_prompt = f"""
VOICE MESSAGE: "{message}"

CURRENT CONTEXT:
- Current Entity/Topic: {context.current_entity or 'None'} 
- Procedure: {context.procedure_type} (Step {context.current_procedure_step}/{context.total_procedure_steps})
- Phase: {context.workflow_phase or 'Main conversation'}
- Last intent: {context.last_intent}
- Hands-free: {context.hands_free_active}
- Error count: {context.error_count}

DETECTED:
- New entity/topic: {detected_entity}
- Topic switch: {is_entity_switch}

CONVERSATION CONTEXT:
- Messages exchanged: {len(context.conversation_history)}
- Entity history: {', '.join(context.entity_history) if context.entity_history else 'None'}
- Topics covered: {', '.join(context.topics_covered) if context.topics_covered else 'None'}

ENHANCED KNOWLEDGE BASE CONTEXT:
{self._format_enhanced_context(enhanced_context) if enhanced_context else "No specific equipment documentation found."}

RELEVANT DOCUMENTS:
{json.dumps(relevant_docs or [], indent=2)}

EXECUTE ALL FOUR CAPABILITIES:
1. Smart Context Awareness - Track equipment, reference history
2. Intelligent Conversation Management - Predict continuation needs  
3. Advanced Voice Orchestration - Multi-phase workflows, conditional logic
4. Hands-Free Optimization - Kitchen environment awareness

RESPOND WITH ALL REQUIRED FIELDS INCLUDING:
- equipment_mentioned, equipment_switch_detected
- procedure_step_info, workflow_phase  
- context_references, response_type
- safety_priority, hands_free_recommendation
"""
            
            # ===================================================================
            # MULTI-AGENT PROCESSING - Enhanced with specialized QSR agents
            # ===================================================================
            
            # STEP 3: Single comprehensive QSR expert agent processing
            # No classification needed - single agent handles all queries
            logger.info(f"ðŸ§  Using comprehensive QSR expert agent for query: {message[:50]}...")
            
            # Process with single comprehensive agent
            if voice_agent:
                # GET MESSAGE HISTORY FOR CONTEXT PERSISTENCE
                message_history = self.get_message_history(session_id)
                logger.info(f"ðŸ§  Running comprehensive QSR expert agent with {len(message_history)} previous messages")
                
                result = await voice_agent.run(user_prompt=enhanced_prompt, message_history=message_history)
                result_data = result.output
                
                # STORE UPDATED MESSAGE HISTORY
                all_messages = result.all_messages()
                logger.info(f"ðŸ’¾ Storing {len(all_messages)} messages (was {len(message_history)})")
                self.store_message_history(session_id, all_messages)
                
                # Verify storage
                stored_messages = self.get_message_history(session_id)
                logger.info(f"âœ… Verified: {len(stored_messages)} messages stored for session {session_id}")
                
                # Extract visual citations from tool usage
                global _last_tool_visual_citations
                if _last_tool_visual_citations:
                    logger.info(f"ðŸ”§ Found visual citations from tool: {len(_last_tool_visual_citations)}")
                    result_data.visual_citations = _last_tool_visual_citations.copy()
                    # Also add to specialized insights for text chat extraction
                    if not result_data.specialized_insights:
                        result_data.specialized_insights = {}
                    result_data.specialized_insights['visual_citations'] = _last_tool_visual_citations.copy()
                    # Clear global variable for next request
                    _last_tool_visual_citations = []
                
                # Mark as comprehensive agent response
                result_data.primary_agent = AgentType.GENERAL
                result_data.coordination_strategy = AgentCoordinationStrategy.SINGLE_AGENT
                logger.info(f"ðŸŽ¯ Comprehensive QSR expert agent processing")
            else:
                # Fallback if no agents available
                result_data = await self._fallback_to_general_agent(message, context, enhanced_context, session_id)
            
            # STEP 3C: Enhanced context updates with multi-agent insights
            self._apply_advanced_context_updates(result_data, context, session_id)
            
            # Add multi-agent specific context updates
            if hasattr(result_data, 'specialized_insights') and result_data.specialized_insights:
                context.context_references.extend([f"Consulted {agent}" for agent in result_data.specialized_insights.keys()])
            
            # STEP 3D: Parse steps for future Playbooks UX (preserved functionality)
            result_data.parsed_steps = self._parse_response_steps(result_data.text_response)
            if result_data.parsed_steps.has_steps:
                logger.info(f"ðŸ“‹ Parsed {result_data.parsed_steps.total_steps} steps: {result_data.parsed_steps.procedure_title}")
            
            # STEP 3E: Enhanced logging with single agent details
            logger.info(f"ðŸŽ¯ Comprehensive QSR expert agent processing complete")
            logger.info(f"ðŸŽ¯ Intent: {result_data.detected_intent}, Equipment: {result_data.equipment_mentioned}")
            logger.info(f"ðŸ”„ Continue listening: {result_data.should_continue_listening}, Hands-free: {result_data.hands_free_recommendation}")
            logger.info(f"âš ï¸ Safety priority: {getattr(result_data, 'safety_priority', False)}")
            
            return result_data
            
        except Exception as e:
            logger.error(f"ðŸš¨ Voice orchestration error: {str(e)}")
            
            # Use intelligent fallback instead of error recovery when entity is detected
            if detected_entity:
                logger.info(f"ðŸ”„ Using intelligent fallback for entity: {detected_entity}")
                return self._intelligent_entity_fallback(message, detected_entity, context, relevant_docs)
            else:
                # Enhanced error recovery with context preservation
                return self._enhanced_error_recovery(message, context)
    
    def _format_enhanced_context(self, enhanced_context: Dict[str, Any]) -> str:
        """Format enhanced context from graph service for LLM processing"""
        if not enhanced_context:
            return "No specific context available."
        
        context_parts = []
        
        # Add query type context
        query_type = enhanced_context.get("query_type", "general")
        if query_type == "temperature":
            context_parts.append("QUERY TYPE: Temperature and food safety requirements")
        elif query_type == "safety":
            context_parts.append("QUERY TYPE: Safety procedures and guidelines")
        
        # Add source content (most important)
        source_content = enhanced_context.get("source_content", [])
        if source_content:
            context_parts.append("RELEVANT MANUAL CONTENT:")
            for i, content in enumerate(source_content[:3], 1):  # Top 3 chunks
                # Clean and format the content
                clean_content = content.replace("\n", " ").strip()
                if len(clean_content) > 300:
                    clean_content = clean_content[:300] + "..."
                context_parts.append(f"{i}. {clean_content}")
        
        # Add page references
        page_refs = enhanced_context.get("page_references", [])
        if page_refs:
            context_parts.append(f"PAGE REFERENCES: {', '.join(map(str, page_refs))}")
        
        # Add equipment context
        equipment = enhanced_context.get("equipment_context")
        if equipment:
            context_parts.append(f"EQUIPMENT CONTEXT: {equipment}")
        
        # Add instruction for LLM
        context_parts.append("\nINSTRUCTION: Use the above manual content to provide a specific, helpful answer about temperature requirements or safety procedures. Extract specific numbers, temperatures, and procedures from the manual content. Do not repeat the raw manual text - synthesize it into a clear, conversational response.")
        
        return "\n".join(context_parts)
    
    def _fallback_response(self, message: str, context: ConversationContext) -> VoiceResponse:
        """Fallback response when PydanticAI agent isn't available"""
        
        # Simple intent detection based on keywords
        message_lower = message.lower()
        
        if any(word in message_lower for word in ["thank", "thanks", "that's all", "goodbye"]):
            intent = ConversationIntent.COMPLETION
            should_continue = False
            complete = True
        elif any(word in message_lower for word in ["what about", "also", "and"]):
            intent = ConversationIntent.FOLLOW_UP
            should_continue = True
            complete = False
        elif any(word in message_lower for word in ["don't understand", "unclear", "what do you mean"]):
            intent = ConversationIntent.CLARIFICATION
            should_continue = True
            complete = False
        else:
            intent = ConversationIntent.EQUIPMENT_QUESTION
            should_continue = True
            complete = False
        
        return VoiceResponse(
            text_response="I understand. Let me help you with that. (Note: PydanticAI agent not available)",
            detected_intent=intent,
            should_continue_listening=should_continue,
            next_voice_state=VoiceState.LISTENING if should_continue else VoiceState.CONVERSATION_COMPLETE,
            confidence_score=0.5,
            conversation_complete=complete,
            suggested_follow_ups=["Is there anything else I can help with?"] if should_continue else []
        )
    
    def end_conversation(self, session_id: str = None):
        """Clean up conversation context"""
        session_id = session_id or self.default_session
        if session_id in self.active_contexts:
            del self.active_contexts[session_id]
    
    def get_conversation_summary(self, session_id: str = None) -> Dict[str, Any]:
        """Get summary of conversation for analytics"""
        context = self.get_context(session_id)
        return {
            "duration": time.time() - context.conversation_start_time,
            "message_count": len(context.conversation_history),
            "topics_covered": context.topics_covered,
            "entities_discussed": context.entity_history,
            "last_intent": context.last_intent,
            "completion_status": context.voice_state == VoiceState.CONVERSATION_COMPLETE,
            "procedure_progress": {
                "type": context.procedure_type,
                "step": context.current_procedure_step,
                "total": context.total_procedure_steps
            }
        }
    
    def _extract_topic_entities(self, message: str, relevant_docs: List[Dict] = None) -> Optional[str]:
        """Extract key topics/entities from user message and relevant documents"""
        message_lower = message.lower()
        
        # First check for common QSR entities/topics from documents
        qsr_entities = {
            # Equipment (order matters - most specific first)
            "ice cream machine": ["ice cream machine", "soft serve machine", "frozen yogurt machine"],
            "fryer": ["fryer", "deep fryer", "fry station"],
            "grill": ["grill", "griddle", "flat top", "char grill"],
            "ice machine": ["ice machine", "ice maker", "ice dispenser"],  # Removed ice cream machine
            "freezer": ["freezer", "walk-in freezer", "freezer unit"],
            "refrigerator": ["refrigerator", "fridge", "cooler", "walk-in cooler"],
            "oven": ["oven", "convection oven", "pizza oven"],
            "dishwasher": ["dishwasher", "dish machine", "warewasher"],
            "coffee machine": ["coffee machine", "coffee maker", "espresso machine"],
            "pos system": ["pos", "point of sale", "register", "cash register"],
            
            # Food safety & procedures
            "cleaning": ["cleaning", "sanitizing", "disinfecting", "hygiene"],
            "food safety": ["food safety", "temperature", "haccp", "contamination"],
            "oil change": ["oil change", "oil replacement", "filter change"],
            "maintenance": ["maintenance", "repair", "service", "inspection"],
            "training": ["training", "orientation", "procedures", "protocol"],
            
            # Ingredients & food items
            "oil": ["oil", "cooking oil", "frying oil"],
            "ingredients": ["ingredients", "recipe", "preparation"],
            "temperature": ["temperature", "temp", "heating", "cooling"],
            
            # Operations
            "opening": ["opening", "start up", "morning routine"],
            "closing": ["closing", "shut down", "end of day"],
            "shift change": ["shift change", "handover", "transition"]
        }
        
        # Check message for entity keywords
        for entity, keywords in qsr_entities.items():
            if any(keyword in message_lower for keyword in keywords):
                return entity
        
        # If no direct match, try to extract from document content
        if relevant_docs:
            for doc in relevant_docs[:2]:  # Check top 2 relevant docs
                content = doc.get('content', '').lower()
                # Look for key nouns/topics in document content
                for entity, keywords in qsr_entities.items():
                    if any(keyword in content for keyword in keywords):
                        return entity
        
        return None
    
    def _detect_entity_switch(self, new_entity: Optional[str], context: ConversationContext) -> bool:
        """Detect if user is switching to different topic/entity"""
        if not new_entity or not context.current_entity:
            return False
        return new_entity != context.current_entity
    
    def _build_context_references(self, context: ConversationContext) -> List[str]:
        """Build references to previous conversation parts"""
        references = []
        
        # Reference to equipment discussions
        if context.entity_history:
            # Filter out None values to prevent join errors
            valid_entities = [entity for entity in context.entity_history if entity is not None]
            if valid_entities:
                references.append(f"Topics discussed: {', '.join(valid_entities)}")
        
        # Reference to procedure progress
        if context.procedure_type and context.current_procedure_step:
            references.append(f"Currently in {context.procedure_type} procedure, step {context.current_procedure_step}")
        
        # Reference to recent topics
        if context.topics_covered:
            references.append(f"Previous topics: {', '.join(context.topics_covered[-2:])}")
        
        return references
    
    def _apply_advanced_context_updates(self, response: VoiceResponse, context: ConversationContext, session_id: str):
        """Apply sophisticated context updates from AI response"""
        
        # Update the last conversation entry with assistant response
        if context.conversation_history:
            context.conversation_history[-1].update({
                "assistant": response.text_response,
                "intent": response.detected_intent,
                "state": response.next_voice_state,
                "equipment": response.equipment_mentioned,
                "response_type": response.response_type,
                "safety_priority": response.safety_priority,
                "procedure_step": response.procedure_step_info
            })
        
        # Entity/Topic tracking (includes equipment and other topics)
        if response.equipment_mentioned:
            if response.equipment_switch_detected:
                # Track topic switch
                context.topic_switches += 1
                context.last_equipment_switch_time = time.time()
                
            # Update current entity (for equipment types)
            context.current_entity = response.equipment_mentioned
            
            # Add to entity history if not already there (and not None)
            if response.equipment_mentioned and response.equipment_mentioned not in context.entity_history:
                context.entity_history.append(response.equipment_mentioned)
        
        # Handle non-equipment entities from context_updates
        if response.context_updates and "current_entity" in response.context_updates:
            entity = response.context_updates["current_entity"]
            context.current_entity = entity
            if entity and entity not in context.entity_history:
                context.entity_history.append(entity)
        
        # Procedure tracking
        if response.procedure_step_info:
            step_info = response.procedure_step_info
            context.current_procedure_step = step_info.get("current_step")
            context.total_procedure_steps = step_info.get("total_steps")
            context.procedure_type = step_info.get("procedure_type")
        
        # Workflow phase tracking
        if response.workflow_phase:
            context.workflow_phase = response.workflow_phase
        
        # State updates
        context.voice_state = response.next_voice_state
        context.last_intent = response.detected_intent
        context.expecting_continuation = response.should_continue_listening
        context.last_response_time = time.time()
        
        # Apply additional context updates
        if response.context_updates:
            self.update_context(session_id, response.context_updates)
        
        # Hands-free management
        context.hands_free_active = response.hands_free_recommendation
        
        # Error tracking
        if response.detected_intent == ConversationIntent.CLARIFICATION:
            context.unclear_responses_count += 1
        else:
            context.unclear_responses_count = 0  # Reset on clear response
    
    def _intelligent_entity_fallback(
        self, 
        message: str, 
        entity: str, 
        context: ConversationContext,
        relevant_docs: List[Dict] = None
    ) -> VoiceResponse:
        """Intelligent fallback for topic/entity-related queries when PydanticAI fails"""
        
        # Generate helpful response based on entity and documents
        if relevant_docs and len(relevant_docs) > 0:
            # Use document information for more informed response
            doc_info = relevant_docs[0].get('content', '')
            if len(doc_info) > 300:
                # Extract first meaningful sentence(s)
                sentences = doc_info.split('. ')
                doc_info = '. '.join(sentences[:2]) + "."
            
            response_text = f"Based on the documentation, here's what I found about {entity}: {doc_info} Would you like me to go into more detail on any specific aspect?"
        else:
            # Generic helpful response based on entity type
            entity_responses = {
                "ice machine": "I can help with your ice machine. What specific issue are you having - not making ice, water leaking, or cleaning maintenance?",
                "fryer": "I can help with your fryer. Are you looking for cleaning steps, oil change procedure, or troubleshooting an issue?",
                "grill": "I can help with your grill. Do you need cleaning instructions, temperature adjustments, or troubleshooting help?",
                "cleaning": "I can help with cleaning procedures. Are you looking for equipment cleaning, food safety protocols, or sanitizing steps?",
                "food safety": "I can help with food safety guidelines. Are you asking about temperatures, storage, or contamination prevention?",
                "oil change": "I can help with oil change procedures. Would you like step-by-step instructions or maintenance schedules?",
                "training": "I can help with training materials. Are you looking for new employee orientation or specific procedure training?",
                "maintenance": "I can help with maintenance procedures. What equipment or system needs attention?"
            }
            
            response_text = entity_responses.get(entity, f"I can help you with {entity}. What specifically would you like to know?")
        
        # Update context
        context.current_entity = entity
        if entity and entity not in context.entity_history:
            context.entity_history.append(entity)
        
        # Parse steps from the response
        parsed_steps = self._parse_response_steps(response_text)
        
        return VoiceResponse(
            text_response=response_text,
            detected_intent=ConversationIntent.EQUIPMENT_QUESTION if entity in ["fryer", "grill", "ice machine"] else ConversationIntent.NEW_TOPIC,
            should_continue_listening=True,
            next_voice_state=VoiceState.LISTENING,
            confidence_score=0.7,
            equipment_mentioned=entity if entity in ["fryer", "grill", "ice machine"] else None,
            response_type="factual",
            hands_free_recommendation=True,
            context_updates={
                "current_entity": entity,
                "current_topic": f"{entity} assistance"
            },
            parsed_steps=parsed_steps
        )

    def _parse_response_steps(self, response_text: str) -> ParsedStepsResponse:
        """Parse steps from AI response for future Playbooks UX integration"""
        try:
            return parse_ai_response_steps(response_text)
        except Exception as e:
            logger.warning(f"Step parsing failed: {str(e)}")
            # Return empty result on parsing failure
            return ParsedStepsResponse(
                original_text=response_text,
                has_steps=False,
                total_steps=0
            )

    def _enhanced_error_recovery(self, message: str, context: ConversationContext) -> VoiceResponse:
        """Enhanced error recovery with context preservation"""
        context.error_count += 1
        
        # Preserve context while recovering
        entity_context = f" about {context.current_entity}" if context.current_entity else ""
        procedure_context = f" in the {context.procedure_type} process" if context.procedure_type else ""
        
        if context.error_count > 2:
            # Suggest alternative interaction mode
            recovery_text = f"I'm having trouble understanding. Should we switch to typing instead? I heard something{entity_context}{procedure_context}."
            should_continue = False
        else:
            # Try to recover with context
            recovery_text = f"I heard something{entity_context}{procedure_context}. What specifically did you need help with?"
            should_continue = True
        
        return VoiceResponse(
            text_response=recovery_text,
            detected_intent=ConversationIntent.CLARIFICATION,
            should_continue_listening=should_continue,
            next_voice_state=VoiceState.ERROR_RECOVERY,
            confidence_score=0.3,
            equipment_mentioned=context.current_entity if context.current_entity in ["fryer", "grill", "ice machine"] else None,
            response_type="clarification",
            hands_free_recommendation=should_continue
        )

# Global orchestrator instance
voice_orchestrator = VoiceOrchestrator()
voice_orchestrator.clear_all_contexts()  # Clear any existing contexts on startup

# Enhanced voice utility functions
def analyze_conversation_flow(context: ConversationContext) -> Dict[str, Any]:
    """Analyze conversation patterns for optimization"""
    recent_intents = [
        msg.get("intent") for msg in context.conversation_history[-3:]
        if msg.get("intent")
    ]
    
    return {
        "recent_intent_pattern": recent_intents,
        "conversation_depth": len(context.conversation_history),
        "topic_switches": len(context.topics_covered),
        "engagement_level": "high" if len(context.conversation_history) > 3 else "standard"
    }

def predict_conversation_end(message: str, context: ConversationContext) -> bool:
    """Predict if conversation is naturally ending"""
    ending_signals = [
        "thank", "thanks", "that's all", "that's it", "good", "perfect",
        "great", "awesome", "got it", "understood", "bye", "goodbye"
    ]
    
    message_lower = message.lower()
    return any(signal in message_lower for signal in ending_signals)

def detect_response_type(message: str, context: ConversationContext) -> str:
    """Detect the type of response needed based on message and context"""
    message_lower = message.lower()
    
    # Safety-related queries
    safety_keywords = ["temperature", "hot", "burn", "electrical", "gas", "chemical", "danger", "safety"]
    if any(keyword in message_lower for keyword in safety_keywords):
        return "safety"
    
    # Procedural queries (cleaning, maintenance, etc.)
    procedure_keywords = ["clean", "maintenance", "repair", "fix", "steps", "how to", "process"]
    if any(keyword in message_lower for keyword in procedure_keywords):
        return "procedural"
    
    # Clarification requests
    clarification_keywords = ["what do you mean", "don't understand", "unclear", "explain", "repeat"]
    if any(keyword in message_lower for keyword in clarification_keywords):
        return "clarification"
    
    # Completion signals
    completion_keywords = ["thanks", "thank you", "that's all", "done", "finished"]
    if any(keyword in message_lower for keyword in completion_keywords):
        return "completion"
    
    # Default to factual
    return "factual"

def should_maintain_hands_free(response_type: str, equipment_mentioned: bool, procedure_active: bool) -> bool:
    """Determine if hands-free mode should stay active"""
    
    # Always maintain for procedural guidance
    if response_type in ["procedural", "safety"] or procedure_active:
        return True
    
    # Maintain if equipment is being discussed (assume worker is at station)
    if equipment_mentioned:
        return True
    
    # End hands-free for completion signals
    if response_type == "completion":
        return False
    
    # Default to maintaining for most interactions
    return True

def extract_procedure_info(message: str, response_text: str) -> Optional[Dict[str, Any]]:
    """Extract procedure step information from response"""
    
    # Look for step patterns in the response
    import re
    
    # Pattern for "Step X of Y" or "X steps total"
    step_pattern = r'(?:step\s+)?(\d+)(?:\s+of\s+(\d+))?'
    step_match = re.search(step_pattern, response_text.lower())
    
    if step_match:
        current_step = int(step_match.group(1))
        total_steps = int(step_match.group(2)) if step_match.group(2) else None
        
        # Determine procedure type
        procedure_type = "general"
        if "clean" in response_text.lower():
            procedure_type = "cleaning"
        elif "maintain" in response_text.lower() or "maintenance" in response_text.lower():
            procedure_type = "maintenance"
        elif "troubleshoot" in response_text.lower() or "fix" in response_text.lower():
            procedure_type = "troubleshooting"
        
        return {
            "current_step": current_step,
            "total_steps": total_steps,
            "procedure_type": procedure_type
        }
    
    return None

def generate_conversation_bridges(response_type: str, equipment: Optional[str], procedure_complete: bool) -> List[str]:
    """Generate natural conversation bridges based on context"""
    bridges = []
    
    if response_type == "procedural":
        if procedure_complete:
            bridges.extend([
                f"All steps done. Any questions about the process?",
                f"That covers the {equipment or 'equipment'} procedure. What else do you need?",
                "Need help with anything else?"
            ])
        else:
            bridges.extend([
                "Ready for the next part?",
                "Need help with any of these steps?",
                "Questions before we continue?"
            ])
    
    elif response_type == "safety":
        bridges.extend([
            "Questions about this before we continue?",
            "Want me to go over the safety steps again?",
            "All clear on the safety requirements?"
        ])
    
    elif response_type == "factual" and equipment:
        bridges.extend([
            f"Any other {equipment} questions?",
            f"What else about the {equipment}?",
            "Need more details on anything?"
        ])
    
    return bridges[:2]  # Return top 2 most relevant bridges